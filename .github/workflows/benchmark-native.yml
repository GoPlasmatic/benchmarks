name: Native Benchmark Pipeline

on:
  workflow_dispatch:
    inputs:
      target_vm_size:
        description: 'Target VM size for Reframe application'
        required: true
        default: 'Standard_B2s'
        type: string
      client_total_requests:
        description: 'Total number of requests per test (client-side)'
        required: false
        default: '10000'
        type: string
      client_concurrency_levels:
        description: 'Comma-separated concurrency levels to test (client-side)'
        required: false
        default: '8,32,64,128'
        type: string
      use_prebuilt:
        description: 'Use pre-built Reframe binary from ACR'
        required: false
        default: false
        type: boolean

env:
  PROJECT_NAME: ${{ vars.PROJECT_NAME }}
  AZURE_LOCATION: ${{ vars.AZURE_LOCATION || 'eastus' }}

jobs:
  provision:
    name: Provision Native VM
    runs-on: ubuntu-latest
    outputs:
      target_vm_ip: ${{ steps.provision.outputs.target_vm_ip }}
      run_id: ${{ steps.provision.outputs.run_id }}
      resource_group: ${{ steps.generate_id.outputs.resource_group }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Generate Run ID
        id: generate_id
        run: |
          echo "run_id=$(date +%Y%m%d%H%M%S)" >> $GITHUB_OUTPUT
          echo "resource_group=benchmark-native-rg-$(date +%Y%m%d%H%M%S)" >> $GITHUB_OUTPUT
      
      - name: Download Reframe Binary from ACR (if using prebuilt)
        if: ${{ inputs.use_prebuilt }}
        run: |
          # This step would download the pre-built binary from ACR
          # Implementation depends on how binaries are stored in ACR
          echo "Downloading pre-built Reframe binary..."
          # docker pull ${ACR_URL}/reframe:latest
          # docker create --name temp ${ACR_URL}/reframe:latest
          # docker cp temp:/app/reframe ./reframe-binary
          # docker rm temp
        env:
          ACR_URL: ${{ secrets.ACR_URL }}
          ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
          ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
      
      - name: Provision Native VM
        id: provision
        run: |
          # Thread count will be automatically set to CPU count on the VM
          chmod +x ./scripts/provision-native-vm.sh
          ./scripts/provision-native-vm.sh \
            "${{ steps.generate_id.outputs.resource_group }}" \
            "${{ env.AZURE_LOCATION }}" \
            "${{ inputs.target_vm_size }}" \
            "Standard_B2s" \
            "${{ steps.generate_id.outputs.run_id }}"
        env:
          ACR_URL: ${{ secrets.ACR_URL }}
          ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
          ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}

  deploy:
    name: Wait for Native Reframe
    runs-on: ubuntu-latest
    needs: provision
    steps:
      - uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Wait for Reframe to be Ready
        timeout-minutes: 15
        run: |
          TARGET_VM_NAME="reframe-target-${{ needs.provision.outputs.run_id }}"
          RESOURCE_GROUP="${{ needs.provision.outputs.resource_group }}"
          echo "Waiting for native Reframe to be healthy on ${TARGET_VM_NAME}..."
          
          for i in {1..90}; do
            echo "Health check attempt $i/90..."
            
            # First check if we can run commands on the VM
            VM_STATUS=$(az vm run-command invoke \
              --resource-group "${RESOURCE_GROUP}" \
              --name "${TARGET_VM_NAME}" \
              --command-id RunShellScript \
              --scripts "echo 'VM accessible'" \
              --query 'value[0].message' -o tsv 2>&1 || echo "VM not accessible")
            
            if [[ "$VM_STATUS" != *"VM accessible"* ]]; then
              echo "VM not yet accessible: $VM_STATUS"
              sleep 10
              continue
            fi
            
            # Check Reframe service status with more details
            SERVICE_STATUS=$(az vm run-command invoke \
              --resource-group "${RESOURCE_GROUP}" \
              --name "${TARGET_VM_NAME}" \
              --command-id RunShellScript \
              --scripts "systemctl is-active reframe || true; systemctl status reframe --no-pager | head -20 || true" \
              --query 'value[0].message' -o tsv 2>/dev/null || echo "")
            echo "Service status: $SERVICE_STATUS"
            
            # Check if reframe binary exists and is executable
            BINARY_CHECK=$(az vm run-command invoke \
              --resource-group "${RESOURCE_GROUP}" \
              --name "${TARGET_VM_NAME}" \
              --command-id RunShellScript \
              --scripts "ls -la /opt/reframe/reframe 2>&1 || echo 'Binary not found'" \
              --query 'value[0].message' -o tsv 2>/dev/null || echo "")
            echo "Binary check: $BINARY_CHECK"
            
            # Check health endpoint
            HEALTH_CHECK=$(az vm run-command invoke \
              --resource-group "${RESOURCE_GROUP}" \
              --name "${TARGET_VM_NAME}" \
              --command-id RunShellScript \
              --scripts "curl -s -o /dev/null -w '%{http_code}' http://localhost:3000/health || echo 'Curl failed'" \
              --query 'value[0].message' -o tsv 2>/dev/null || echo "")
            
            if [[ "$HEALTH_CHECK" == *"200"* ]]; then
              echo "Native Reframe is healthy!"
              
              # Get configuration info
              echo "Getting Reframe configuration..."
              az vm run-command invoke \
                --resource-group "${RESOURCE_GROUP}" \
                --name "${TARGET_VM_NAME}" \
                --command-id RunShellScript \
                --scripts "curl -s http://localhost:3000/health | jq . || echo 'No health data'" \
                --query 'value[0].message' -o tsv
              
              # Show thread configuration
              echo "Thread configuration:"
              az vm run-command invoke \
                --resource-group "${RESOURCE_GROUP}" \
                --name "${TARGET_VM_NAME}" \
                --command-id RunShellScript \
                --scripts "cat /etc/environment | grep REFRAME || echo 'No REFRAME vars'; nproc" \
                --query 'value[0].message' -o tsv
              
              exit 0
            fi
            echo "Health check response: $HEALTH_CHECK"
            sleep 10
          done
          
          echo "Reframe failed to become healthy"
          # Get diagnostic information
          echo "Getting diagnostic information..."
          echo "=== Reframe Service Logs ==="
          az vm run-command invoke \
            --resource-group "${RESOURCE_GROUP}" \
            --name "${TARGET_VM_NAME}" \
            --command-id RunShellScript \
            --scripts "journalctl -u reframe -n 100 --no-pager" \
            --query 'value[0].message' -o tsv || echo "Could not get logs"
          
          echo "=== Reframe Binary and Environment ==="
          az vm run-command invoke \
            --resource-group "${RESOURCE_GROUP}" \
            --name "${TARGET_VM_NAME}" \
            --command-id RunShellScript \
            --scripts "ls -la /opt/reframe/; cat /opt/reframe/reframe.env 2>/dev/null || echo 'No env file'; nproc" \
            --query 'value[0].message' -o tsv || echo "Could not check environment"
          
          exit 1

  benchmark:
    name: Run Native Benchmark
    runs-on: ubuntu-latest
    needs: [provision, deploy]
    steps:
      - uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Execute Native Benchmark
        id: benchmark
        timeout-minutes: 30
        run: |
          TARGET_VM_NAME="reframe-target-${{ needs.provision.outputs.run_id }}"
          RESOURCE_GROUP="${{ needs.provision.outputs.resource_group }}"
          
          echo "Starting native benchmark..."
          echo "Target VM: ${TARGET_VM_NAME}"
          echo "Resource Group: ${RESOURCE_GROUP}"
          echo "Client total requests: ${{ inputs.client_total_requests }}"
          echo "Client concurrency levels: ${{ inputs.client_concurrency_levels }}"
          
          # First, verify the VM is accessible
          echo "Verifying VM is accessible..."
          az vm run-command invoke \
            --resource-group "${RESOURCE_GROUP}" \
            --name "${TARGET_VM_NAME}" \
            --command-id RunShellScript \
            --scripts "echo 'VM is accessible' && ls -la /opt/benchmark/" \
            --query 'value[0].message' -o tsv
          
          # Record start time for metrics collection
          BENCHMARK_START=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          echo "benchmark_start=${BENCHMARK_START}" >> $GITHUB_OUTPUT
          
          # Parse configs and run each separately
          IFS=',' read -ra CONFIGS <<< "${{ inputs.client_concurrency_levels }}"
          
          # Initialize combined results
          echo '{"results": []}' > benchmark_results.json
          
          for CONFIG in "${CONFIGS[@]}"; do
            CONFIG=$(echo $CONFIG | xargs) # Trim whitespace
            echo "Running benchmark with concurrency: ${CONFIG}"
            
            # Run single config benchmark (shorter timeout per config)
            timeout 300 az vm run-command invoke \
              --resource-group "${RESOURCE_GROUP}" \
              --name "${TARGET_VM_NAME}" \
              --command-id RunShellScript \
              --scripts "cd /opt/benchmark && \
                REFRAME_URL=http://localhost:3000 \
                BENCHMARK_REQUESTS=${{ inputs.client_total_requests }} \
                BENCHMARK_CONFIGS='${CONFIG}' \
                BENCHMARK_WARMUP=10 \
                python3 benchmark.py" \
              --query 'value[0].message' -o tsv > "benchmark_${CONFIG}.txt" || {
                echo "Warning: Benchmark for config ${CONFIG} failed or timed out"
                continue
              }
            
            # Extract JSON for this config
            sed -n '/JSON_OUTPUT_START/,/JSON_OUTPUT_END/p' "benchmark_${CONFIG}.txt" | sed '1d;$d' > "benchmark_${CONFIG}.json"
            
            # Show progress
            if [ -s "benchmark_${CONFIG}.json" ]; then
              echo "Config ${CONFIG} completed successfully"
              # Extract key metrics for quick view
              jq -r '.results[0] | "  Throughput: \(.throughput) req/s, P99 Latency: \(.latency.p99) ms"' "benchmark_${CONFIG}.json" || true
            else
              echo "Config ${CONFIG} - no valid results"
            fi
          done
          
          # Record end time for metrics collection
          BENCHMARK_END=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          echo "benchmark_end=${BENCHMARK_END}" >> $GITHUB_OUTPUT
          
          # Combine all results into single JSON
          echo "Combining benchmark results..."
          python3 - << 'EOF'
          import json
          import glob
          import os
          from datetime import datetime
          
          all_results = []
          config_files = glob.glob("benchmark_*.json")
          
          for file in config_files:
              if os.path.getsize(file) > 0:
                  try:
                      with open(file, 'r') as f:
                          data = json.load(f)
                          if 'results' in data and data['results']:
                              all_results.extend(data['results'])
                  except:
                      print(f"Warning: Could not parse {file}")
          
          # Create combined output
          output = {
              'timestamp': datetime.utcnow().isoformat() + 'Z',
              'configuration': {
                  'target_url': 'http://localhost:3000',
                  'total_requests': ${{ inputs.client_total_requests }},
                  'concurrency_levels': '${{ inputs.client_concurrency_levels }}'.split(',')
              },
              'results': all_results
          }
          
          # Add summary if we have results
          if all_results:
              best_throughput = max(all_results, key=lambda x: x.get('throughput', 0))
              best_latency = min(all_results, key=lambda x: x.get('latency', {}).get('p99', float('inf')))
              
              output['summary'] = {
                  'best_throughput': {
                      'configuration': best_throughput.get('configuration'),
                      'value': best_throughput.get('throughput')
                  },
                  'best_p99_latency': {
                      'configuration': best_latency.get('configuration'),
                      'value': best_latency.get('latency', {}).get('p99')
                  }
              }
          
          with open('benchmark_results.json', 'w') as f:
              json.dump(output, f, indent=2)
          
          print(f"Combined {len(all_results)} benchmark results")
          EOF
          
          # Display summary
          echo "Benchmark completed. Summary:"
          jq '.summary' benchmark_results.json || echo "No summary available"
      
      - name: Collect System Metrics
        id: metrics
        run: |
          TARGET_VM_NAME="reframe-target-${{ needs.provision.outputs.run_id }}"
          RESOURCE_GROUP="${{ needs.provision.outputs.resource_group }}"
          
          # Get system info
          echo "Collecting system information..."
          az vm run-command invoke \
            --resource-group "${RESOURCE_GROUP}" \
            --name "${TARGET_VM_NAME}" \
            --command-id RunShellScript \
            --scripts "echo 'CPU Info:' && lscpu | grep -E 'Model name|CPU\(s\)|Thread|Core|Socket' && \
                      echo && echo 'Memory Info:' && free -h && \
                      echo && echo 'Process Info:' && ps aux | grep reframe | grep -v grep" \
            --query 'value[0].message' -o tsv
          
          # Get Azure metrics
          SUBSCRIPTION_ID=$(az account show --query id -o tsv)
          START_TIME="${{ steps.benchmark.outputs.benchmark_start }}"
          END_TIME="${{ steps.benchmark.outputs.benchmark_end }}"
          
          echo "Collecting CPU metrics from ${START_TIME} to ${END_TIME}"
          
          # Wait for metrics to be available
          sleep 30
          
          az monitor metrics list \
            --resource "/subscriptions/${SUBSCRIPTION_ID}/resourceGroups/${{ needs.provision.outputs.resource_group }}/providers/Microsoft.Compute/virtualMachines/${TARGET_VM_NAME}" \
            --metric "Percentage CPU" \
            --start-time "${START_TIME}" \
            --end-time "${END_TIME}" \
            --interval PT1M \
            --aggregation Average Maximum \
            --output json > cpu_metrics.json || echo '{"value":[]}' > cpu_metrics.json
          
          # Extract metrics
          AVG_CPU=$(jq 'if .value[0].timeseries[0].data then ([.value[0].timeseries[0].data[].average | select(. != null)] | if length > 0 then add/length else 0 end) else 0 end' cpu_metrics.json || echo "0")
          PEAK_CPU=$(jq 'if .value[0].timeseries[0].data then ([.value[0].timeseries[0].data[].maximum | select(. != null)] | if length > 0 then max else 0 end) else 0 end' cpu_metrics.json || echo "0")
          
          echo "Average CPU: ${AVG_CPU}%"
          echo "Peak CPU: ${PEAK_CPU}%"
          
          echo "avg_cpu=${AVG_CPU}" >> $GITHUB_OUTPUT
          echo "peak_cpu=${PEAK_CPU}" >> $GITHUB_OUTPUT
      
      - name: Generate Report
        id: report
        run: |
          RUN_ID="${{ needs.provision.outputs.run_id }}"
          VM_SIZE="${{ inputs.target_vm_size }}"
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          
          # Parse benchmark results
          if [ -f benchmark_results.json ] && [ -s benchmark_results.json ]; then
            BENCHMARK_DATA=$(cat benchmark_results.json)
          else
            BENCHMARK_DATA='{"error": "No benchmark results found"}'
          fi
          
          # Create comprehensive report
          cat > report.json << EOF
          {
            "run_id": "${RUN_ID}",
            "timestamp": "${TIMESTAMP}",
            "deployment": "native",
            "configuration": {
              "target_vm_size": "${VM_SIZE}",
              "reframe_thread_count": "auto (CPU count)",
              "client_total_requests": ${{ inputs.client_total_requests }},
              "client_concurrency_levels": "${{ inputs.client_concurrency_levels }}"
            },
            "metrics": {
              "cpu": {
                "average": ${{ steps.metrics.outputs.avg_cpu }},
                "peak": ${{ steps.metrics.outputs.peak_cpu }}
              }
            },
            "benchmark_results": ${BENCHMARK_DATA}
          }
          EOF
          
          # Save report
          REPORT_NAME="benchmark_native_${VM_SIZE}_$(date +%Y%m%d)_${RUN_ID}.json"
          mkdir -p reports
          cp report.json "reports/${REPORT_NAME}"
          
          echo "Report generated: ${REPORT_NAME}"
          jq '.benchmark_results.summary' report.json
          
          echo "report_name=${REPORT_NAME}" >> $GITHUB_OUTPUT
      
      - name: Commit Report
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          if [ -f "reports/${{ steps.report.outputs.report_name }}" ]; then
            git add -f "reports/${{ steps.report.outputs.report_name }}"
            git commit -m "Add native benchmark report: ${{ steps.report.outputs.report_name }}" || echo "No changes to commit"
            git push || echo "Nothing to push"
          fi

  cleanup:
    name: Cleanup Resources
    runs-on: ubuntu-latest
    needs: [provision, benchmark]
    if: always()
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Delete Resource Group
        run: |
          az group delete \
            --name "${{ needs.provision.outputs.resource_group }}" \
            --yes \
            --no-wait
          
          echo "Resource group deletion initiated. All resources will be removed."