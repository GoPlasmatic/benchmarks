name: Benchmark Pipeline

on:
  workflow_dispatch:
    inputs:
      target_vm_size:
        description: 'Target VM size for Reframe application (e.g., Standard_B2s, Standard_D16s_v5)'
        required: true
        default: 'Standard_B2s'
        type: string
      client_total_requests:
        description: 'Total number of requests per test (client-side)'
        required: false
        default: '100000'
        type: string
      client_concurrent_connections:
        description: 'Number of concurrent connections (client-side)'
        required: false
        default: '128'
        type: string
      client_concurrency_levels:
        description: 'Comma-separated concurrency levels to test (client-side)'
        required: false
        default: '8,12,16'
        type: string
      reframe_version:
        description: 'Reframe Docker image tag'
        required: false
        default: 'latest'
        type: string

env:
  PROJECT_NAME: ${{ vars.PROJECT_NAME }}
  BENCHMARK_VM_SIZE: ${{ vars.BENCHMARK_VM_SIZE || 'Standard_B2s' }}
  AZURE_LOCATION: ${{ vars.AZURE_LOCATION || 'eastus' }}

jobs:
  provision:
    name: Provision Infrastructure
    runs-on: ubuntu-latest
    outputs:
      target_vm_ip: ${{ steps.provision.outputs.target_vm_ip }}
      run_id: ${{ steps.provision.outputs.run_id }}
      resource_group: ${{ steps.generate_id.outputs.resource_group }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Generate Run ID
        id: generate_id
        run: |
          echo "run_id=$(date +%Y%m%d%H%M%S)" >> $GITHUB_OUTPUT
          echo "resource_group=benchmark-rg-$(date +%Y%m%d%H%M%S)" >> $GITHUB_OUTPUT
      
      - name: Provision VMs
        id: provision
        run: |
          chmod +x ./scripts/provision-vms.sh
          ./scripts/provision-vms.sh \
            "${{ steps.generate_id.outputs.resource_group }}" \
            "${{ env.AZURE_LOCATION }}" \
            "${{ inputs.target_vm_size }}" \
            "${{ env.BENCHMARK_VM_SIZE }}" \
            "${{ steps.generate_id.outputs.run_id }}"
        env:
          ACR_URL: ${{ secrets.ACR_URL }}
          ACR_USERNAME: ${{ secrets.ACR_USERNAME }}
          ACR_PASSWORD: ${{ secrets.ACR_PASSWORD }}
          REFRAME_VERSION: ${{ inputs.reframe_version }}

  deploy:
    name: Deploy Reframe Application
    runs-on: ubuntu-latest
    needs: provision
    steps:
      - uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Wait for Reframe Deployment
        run: |
          TARGET_VM_NAME="reframe-target-${{ needs.provision.outputs.run_id }}"
          RESOURCE_GROUP="${{ needs.provision.outputs.resource_group }}"
          echo "Waiting for Reframe to be healthy on ${TARGET_VM_NAME}..."
          
          for i in {1..60}; do
            HEALTH_CHECK=$(az vm run-command invoke \
              --resource-group "${RESOURCE_GROUP}" \
              --name "${TARGET_VM_NAME}" \
              --command-id RunShellScript \
              --scripts "curl -s -o /dev/null -w '%{http_code}' http://localhost:3000/health" \
              --query 'value[0].message' -o tsv 2>/dev/null || echo "")
            
            if [[ "$HEALTH_CHECK" == *"200"* ]]; then
              echo "Reframe is healthy!"
              exit 0
            fi
            echo "Attempt $i/60: Reframe not ready yet..."
            sleep 10
          done
          
          echo "Reframe failed to become healthy"
          exit 1

  benchmark:
    name: Run Benchmark
    runs-on: ubuntu-latest
    needs: [provision, deploy]
    steps:
      - uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Execute Benchmark
        id: benchmark
        timeout-minutes: 30
        run: |
          TARGET_VM_NAME="reframe-target-${{ needs.provision.outputs.run_id }}"
          RESOURCE_GROUP="${{ needs.provision.outputs.resource_group }}"
          
          echo "Starting benchmark on target VM using docker-compose..."
          echo "Client total requests: ${{ inputs.client_total_requests }}"
          echo "Client concurrency levels: ${{ inputs.client_concurrency_levels }}"
          
          # Record start time for metrics collection
          BENCHMARK_START=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          echo "benchmark_start=${BENCHMARK_START}" >> $GITHUB_OUTPUT
          
          # Parse configs and run each separately
          IFS=',' read -ra CONFIGS <<< "${{ inputs.client_concurrency_levels }}"
          
          # Initialize combined results
          echo '{"results": []}' > benchmark_results.json
          
          for CONFIG in "${CONFIGS[@]}"; do
            CONFIG=$(echo $CONFIG | xargs) # Trim whitespace
            echo "Running benchmark with concurrency: ${CONFIG}"
            
            # Run single config benchmark using docker-compose
            timeout 300 az vm run-command invoke \
              --resource-group "${RESOURCE_GROUP}" \
              --name "${TARGET_VM_NAME}" \
              --command-id RunShellScript \
              --scripts "cd /opt/reframe && \
                BENCHMARK_REQUESTS=${{ inputs.client_total_requests }} \
                BENCHMARK_CONFIGS='${CONFIG}' \
                docker-compose --profile benchmark run --rm benchmark-runner" \
              --query 'value[0].message' -o tsv > "benchmark_${CONFIG}.txt" || {
                echo "Warning: Benchmark for config ${CONFIG} failed or timed out"
                continue
              }
            
            # Extract JSON for this config
            sed -n '/JSON_OUTPUT_START/,/JSON_OUTPUT_END/p' "benchmark_${CONFIG}.txt" | sed '1d;$d' > "benchmark_${CONFIG}.json"
            
            # Show progress
            if [ -s "benchmark_${CONFIG}.json" ]; then
              echo "Config ${CONFIG} completed successfully"
              jq -r '.results[0] | "  Throughput: \(.throughput) req/s, P99 Latency: \(.latency.p99) ms"' "benchmark_${CONFIG}.json" || true
            else
              echo "Config ${CONFIG} - no valid results"
            fi
          done
          
          # Record end time for metrics collection
          BENCHMARK_END=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          echo "benchmark_end=${BENCHMARK_END}" >> $GITHUB_OUTPUT
          
          # Combine all results into single JSON
          echo "Combining benchmark results..."
          python3 - << 'EOF'
          import json
          import glob
          import os
          from datetime import datetime
          
          all_results = []
          config_files = glob.glob("benchmark_*.json")
          
          for file in config_files:
              if os.path.getsize(file) > 0:
                  try:
                      with open(file, 'r') as f:
                          data = json.load(f)
                          if 'results' in data and data['results']:
                              all_results.extend(data['results'])
                  except:
                      print(f"Warning: Could not parse {file}")
          
          # Create combined output
          output = {
              'timestamp': datetime.utcnow().isoformat() + 'Z',
              'configuration': {
                  'target_url': 'http://localhost:3000',
                  'total_requests': ${{ inputs.client_total_requests }},
                  'concurrency_levels': '${{ inputs.client_concurrency_levels }}'.split(',')
              },
              'results': all_results
          }
          
          # Add summary if we have results
          if all_results:
              best_throughput = max(all_results, key=lambda x: x.get('throughput', 0))
              best_latency = min(all_results, key=lambda x: x.get('latency', {}).get('p99', float('inf')))
              
              output['summary'] = {
                  'best_throughput': {
                      'configuration': best_throughput.get('configuration'),
                      'value': best_throughput.get('throughput')
                  },
                  'best_p99_latency': {
                      'configuration': best_latency.get('configuration'),
                      'value': best_latency.get('latency', {}).get('p99')
                  }
              }
          
          with open('benchmark_results.json', 'w') as f:
              json.dump(output, f, indent=2)
          
          print(f"Combined {len(all_results)} benchmark results")
          EOF
          
          # Display summary
          echo "Benchmark completed. Summary:"
          jq '.summary' benchmark_results.json || echo "No summary available"
      
      - name: Collect Azure Metrics
        id: metrics
        run: |
          TARGET_VM_NAME="reframe-target-${{ needs.provision.outputs.run_id }}"
          
          # Get subscription ID from Azure CLI
          SUBSCRIPTION_ID=$(az account show --query id -o tsv)
          
          # Use benchmark execution time window for metrics
          START_TIME="${{ steps.benchmark.outputs.benchmark_start }}"
          END_TIME="${{ steps.benchmark.outputs.benchmark_end }}"
          
          echo "Collecting CPU metrics from ${START_TIME} to ${END_TIME}"
          
          # Wait a bit for metrics to be available in Azure Monitor
          sleep 30
          
          az monitor metrics list \
            --resource "/subscriptions/${SUBSCRIPTION_ID}/resourceGroups/${{ needs.provision.outputs.resource_group }}/providers/Microsoft.Compute/virtualMachines/${TARGET_VM_NAME}" \
            --metric "Percentage CPU" \
            --start-time "${START_TIME}" \
            --end-time "${END_TIME}" \
            --interval PT1M \
            --aggregation Average Maximum \
            --output json > cpu_metrics.json || echo '{"value":[]}' > cpu_metrics.json
          
          # Debug: Show raw metrics
          echo "Raw metrics data:"
          jq '.value[0].timeseries[0].data' cpu_metrics.json || echo "No metrics data"
          
          # Extract average and peak CPU (with defaults if no data)
          AVG_CPU=$(jq 'if .value[0].timeseries[0].data then ([.value[0].timeseries[0].data[].average | select(. != null)] | if length > 0 then add/length else 0 end) else 0 end' cpu_metrics.json || echo "0")
          PEAK_CPU=$(jq 'if .value[0].timeseries[0].data then ([.value[0].timeseries[0].data[].maximum | select(. != null)] | if length > 0 then max else 0 end) else 0 end' cpu_metrics.json || echo "0")
          
          echo "Average CPU: ${AVG_CPU}%"
          echo "Peak CPU: ${PEAK_CPU}%"
          
          echo "avg_cpu=${AVG_CPU}" >> $GITHUB_OUTPUT
          echo "peak_cpu=${PEAK_CPU}" >> $GITHUB_OUTPUT
      
      - name: Generate Report
        id: report
        run: |
          RUN_ID="${{ needs.provision.outputs.run_id }}"
          VM_SIZE="${{ inputs.target_vm_size }}"
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          
          # Parse benchmark results
          if [ -f benchmark_results.json ] && [ -s benchmark_results.json ]; then
            BENCHMARK_DATA=$(cat benchmark_results.json)
          else
            BENCHMARK_DATA='{"error": "No benchmark results found"}'
          fi
          
          # Create comprehensive report
          cat > report.json << EOF
          {
            "run_id": "${RUN_ID}",
            "timestamp": "${TIMESTAMP}",
            "configuration": {
              "target_vm_size": "${VM_SIZE}",
              "benchmark_vm_size": "${{ env.BENCHMARK_VM_SIZE }}",
              "reframe_version": "${{ inputs.reframe_version }}",
              "reframe_thread_count": "auto (CPU count)",
              "client_total_requests": ${{ inputs.client_total_requests }},
              "client_concurrency_levels": "${{ inputs.client_concurrency_levels }}"
            },
            "metrics": {
              "cpu": {
                "average": ${{ steps.metrics.outputs.avg_cpu }},
                "peak": ${{ steps.metrics.outputs.peak_cpu }}
              }
            },
            "benchmark_results": ${BENCHMARK_DATA}
          }
          EOF
          
          # Save report with naming convention
          REPORT_NAME="benchmark_${VM_SIZE}_$(date +%Y%m%d)_${RUN_ID}.json"
          mkdir -p reports
          cp report.json "reports/${REPORT_NAME}"
          
          # Display summary
          echo "Report generated: ${REPORT_NAME}"
          jq '.benchmark_results.summary' report.json
          
          echo "report_name=${REPORT_NAME}" >> $GITHUB_OUTPUT
      
      - name: Commit Report
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Ensure reports directory exists and has the report
          if [ -f "reports/${{ steps.report.outputs.report_name }}" ]; then
            git add -f "reports/${{ steps.report.outputs.report_name }}"
            git commit -m "Add benchmark report: ${{ steps.report.outputs.report_name }}" || echo "No changes to commit"
            git push || echo "Nothing to push"
          else
            echo "Report file not found: reports/${{ steps.report.outputs.report_name }}"
            exit 1
          fi

  cleanup:
    name: Cleanup Resources
    runs-on: ubuntu-latest
    needs: [provision, benchmark]
    if: always()
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Delete Resource Group
        run: |
          # Delete all resources by deleting the resource group
          az group delete \
            --name "${{ needs.provision.outputs.resource_group }}" \
            --yes \
            --no-wait
          
          echo "Resource group deletion initiated. All resources will be removed."